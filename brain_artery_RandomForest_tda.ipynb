{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study case: Brain vasculature in acute ischemic stroke\n",
    "\n",
    "##### License: Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded giotto!\n"
     ]
    }
   ],
   "source": [
    "# Load giotto packages\n",
    "import sys\n",
    "sys.path.insert(0, './giotto-tda')\n",
    "from gtda.images import Binarizer, Inverter, HeightFiltration, RadialFiltration\n",
    "from gtda.homology import VietorisRipsPersistence, CubicalPersistence\n",
    "from gtda.diagrams import  PairwiseDistance, Amplitude, Scaler, PersistenceEntropy, BettiCurve, PersistenceLandscape, HeatKernel\n",
    "from BnDs.analysis.utils.ImageToPointCloud import ImageToPointCloud\n",
    "\n",
    "print('Successfully loaded giotto!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BnDs specific functions\n",
    "from BnDs.analysis import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generic packages\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import clone\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the stroke dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a total of 113 subjects.\n",
      "Sequences used: {'ct_sequences': ['wmask_filtered_extracted_betted_Angio'], 'ct_label_sequences': ['wcoreg_VOI'], 'mri_sequences': [], 'mri_label_sequences': []}\n",
      "0 subjects had been excluded.\n",
      "(10, 79, 95, 79) (10,)\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "data_dir = './data/'\n",
    "clinical_inputs, ct_inputs, ct_lesion_GT, mri_inputs, mri_lesion_GT, brain_masks, ids, params = dl.load_structured_data(data_dir, 'data_set.npz')\n",
    "lesion_volumes = np.sum(ct_lesion_GT, axis=(1, 2, 3))\n",
    "\n",
    "n_subj, n_x, n_y, n_z, n_channels = ct_inputs.shape\n",
    "X = np.squeeze(ct_inputs)\n",
    "y = lesion_volumes\n",
    "\n",
    "X = X[:10]\n",
    "y = y[:10]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(np.min(X), np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 79, 95, 79) (8,) (2, 79, 95, 79) (2,)\n"
     ]
    }
   ],
   "source": [
    "# Set up the data\n",
    "n_train, n_test = int(X.shape[0] * 0.8), int(X.shape[0] * 0.2)\n",
    "\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:n_train+n_test]\n",
    "y_test = y[n_train:n_train+n_test]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directions and centers along every dimension\n",
    "direction_list = [list(p) for p in itertools.product([1, 0, -1], repeat=3)]\n",
    "direction_list.remove([0, 0, 0])\n",
    "center_per_dim = [[int(0.25*n_x), int(0.5*n_x), int(0.75*n_x)], \n",
    "                  [int(0.25*n_y), int(0.5*n_y), int(0.75*n_y)], \n",
    "                  [int(0.25*n_z), int(0.5*n_z), int(0.75*n_z)]]\n",
    "center_list = [list(p) for p in itertools.product(*center_per_dim)]\n",
    "\n",
    "\n",
    "filtration_list = \\\n",
    " [HeightFiltration(direction=direction) \n",
    "   for direction in direction_list] + \\\n",
    " [RadialFiltration(center=center) \n",
    "   for center in center_list] + \\\n",
    " [None]\n",
    "\n",
    "binarizer = Binarizer(threshold=0.4)\n",
    "point = ImageToPointCloud()\n",
    "cubical = CubicalPersistence(homology_dimensions=[0, 1])\n",
    "rips = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "scaler = Scaler(metric='bottleneck')\n",
    "\n",
    "grayscale_steps = [[cubical, scaler]]\n",
    "filtration_steps = [[binarizer, filtration, cubical, scaler] for filtration in filtration_list]\n",
    "# Point step provokes:\n",
    "# ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "# Cause: Point gives infinity for all values where there is no point in order to conserve the dimensions of the array\n",
    "# Temporary solution: remove check_array from VietorisRips function responsible for checking for Inf\n",
    "rips_steps = [[binarizer, point, rips, scaler]]\n",
    "\n",
    "image_steps = grayscale_steps + filtration_steps + rips_steps\n",
    "\n",
    "metric_list = [ \n",
    "   {'metric': 'bottleneck', 'metric_params': {'p': np.inf}},\n",
    "   {'metric': 'wasserstein', 'metric_params': {'p': 1}},\n",
    "   {'metric': 'wasserstein', 'metric_params': {'p': 2}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 1, 'n_layers': 1, 'n_values': 100}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 1, 'n_layers': 2, 'n_values': 100}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 2, 'n_layers': 1, 'n_values': 100}},\n",
    "   {'metric': 'landscape', 'metric_params': {'p': 2, 'n_layers': 2, 'n_values': 100}},\n",
    "   {'metric': 'betti', 'metric_params': {'p': 1, 'n_values': 100}},\n",
    "   {'metric': 'betti', 'metric_params': {'p': 2, 'n_values': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 1, 'sigma': 1.6, 'n_values': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 1, 'sigma': 3.2, 'n_values': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 2, 'sigma': 1.6, 'n_values': 100}},\n",
    "   {'metric': 'heat', 'metric_params': {'p': 2, 'sigma': 3.2, 'n_values': 100}}\n",
    "]\n",
    "\n",
    "entropy_steps = [steps + [PersistenceEntropy()] for steps in image_steps]\n",
    "amplitude_steps = [steps+[Amplitude(**metric, order=None)] for steps in image_steps for metric in metric_list]\n",
    "\n",
    "all_steps = entropy_steps + amplitude_steps\n",
    "feature_union = make_union(*[make_pipeline(*steps) for steps in all_steps], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tda = feature_union.fit_transform(X_train, y_train)\n",
    "pkl.dump(X_train_tda, open('X_train_tda.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tda = feature_union.transform(X_test)\n",
    "pkl.dump(X_test_tda, open('X_test_tda.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10000, n_jobs=-1)\n",
    "classifier.fit(X_train_tda, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = classifier.predict(X_train_tda)\n",
    "pkl.dump(y_train_predict, open('y_train_predict_tda.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = classifier.predict(X_test_tda)\n",
    "pkl.dump(y_test_predict, open('y_test_predict_tda.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_test_predict)\n",
    "plt.imshow(confusion)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = -classifier.feature_importances_\n",
    "importance_indices = np.argsort(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_importance = X_train_tda[:, importance_indices]\n",
    "print(X_train_importance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = np.abs(np.corrcoef(X_train_tda.T))\n",
    "plt.imshow(correlation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(correlation_, importance_indices_, threshold_):\n",
    "    decorrelated_index = importance_indices_[:1]\n",
    "\n",
    "    for index in importance_indices_[1:]:\n",
    "        if np.sum(correlation_[decorrelated_index, index] > threshold_) == 0:\n",
    "            decorrelated_index = np.append(decorrelated_index, [index])\n",
    "    return decorrelated_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_scores(thresholds_list_, n_features_list_, correlation_, importance_indices_, postfix, n_folds=3):\n",
    "    for threshold_ in thresholds_list_:\n",
    "        print('threshold: ', threshold_)\n",
    "        decorrelated_feature_index = select_features(correlation_, importance_indices_, threshold_=threshold_)\n",
    "        n_features_max = decorrelated_feature_index.shape[0]\n",
    "        print('Number of decorrelated features: ', n_features_max)\n",
    "        n_features_list = sorted(list(set([n_features if n_features <= n_features_max else n_features_max\n",
    "                               for n_features in n_features_list_])))\n",
    "        print('n_features_list: ', n_features_list)\n",
    "\n",
    "        cv_scores = {}\n",
    "        cv = KFold(n_folds)\n",
    "        for n_features in n_features_list:\n",
    "            X_train_n_features = X_train_tda[:, decorrelated_feature_index[:n_features]]\n",
    "            print('X_train shape: ', X_train_n_features.shape)\n",
    "            cv_scores[n_features] = cross_val_score(classifier, \n",
    "                                                    X_train_n_features, \n",
    "                                                    y_train, cv=cv)\n",
    "            print('cv scores for', postfix, ': ', n_features, cv_scores[n_features])\n",
    "        pkl.dump(cv_scores, open('cv_scores_'+str(threshold_)+'_'+postfix+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_list = [0.8, 0.9, 0.95, 1.0]\n",
    "n_features_list = [4, 8, 28, 42, 56, 112, 178, 244, 392, 784]\n",
    "run_cv_scores(thresholds_list, n_features_list, correlation, importance_indices, 'tda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (BnDs)",
   "language": "python",
   "name": "pycharm-f52dae77"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
