{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature Creation, Classification and Scoring\n",
    "\n",
    "> \"Going the whole way\", *Churchill, 2048*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "path_bnd = '../'\n",
    "sys.path.insert(1, path_bnd)\n",
    "import analysis_tools.data_loader as dl\n",
    "from gsprep.visual_tools.visual import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gtda.homology import CubicalPersistence\n",
    "from pgtda.diagrams import  PersistenceEntropy, Amplitude, Filtering, Scaler\n",
    "from pgtda.plotting import plot_diagram\n",
    "from pgtda.images import Inverter, Padder\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "from pgtda.images import RollingSubImageTransformer, make_image_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/julian/stroke_research/brain_and_donuts/full_datasets'\n",
    "clinical_inputs, ct_inputs, ct_lesion_GT, mri_inputs, mri_lesion_GT, brain_masks, ids, params = \\\n",
    "dl.load_structured_data(data_dir, 'withAngio_all_2016_2017.npz')\n",
    "\n",
    "# Reshape ct_inputs as it has 1 channel\n",
    "ct_inputs = ct_inputs.reshape((*ct_inputs.shape[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data exploration set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data subset\n",
    "n_images = 1\n",
    "X = (ct_inputs[:n_images] * brain_masks[:n_images])[range(n_images), ::2, ::2, ::2]\n",
    "y = (ct_lesion_GT[:n_images] * brain_masks[:n_images])[range(n_images), ::2, ::2, ::2]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(X[0])\n",
    "display(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    " \n",
    "Example pipeline: Get persistence entropies and wasserstein amplitude per subwindow for subwindows of different width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "width_list = [[5, 5, 5], [7, 7, 7]]\n",
    "# Note that padding should be same so that output images always have the same size\n",
    "transformer = make_pipeline(CubicalPersistence(homology_dimensions=(0, 1 ,2), n_jobs=-1), \n",
    "                             make_union(PersistenceEntropy(n_jobs=-1), \n",
    "                                         Amplitude(metric='wasserstein', metric_params={'p':2}, order=None, n_jobs=-1)))\n",
    "rsis = make_image_union(*[RollingSubImageTransformer(transformer=transformer, width=width, padding='same')\n",
    "                    for width in width_list], n_jobs=-1)\n",
    "X_subfeatures_union = rsis.fit_transform(X)\n",
    "print(X_subfeatures_union.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the persistence entropies and the amplitudes for subwindows of various sizes\n",
    "display(X_subfeatures_union[0, :, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = X_subfeatures_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just cheating here to mimick more subjects\n",
    "X_features = np.concatenate([X_features, X_features], axis = 0)\n",
    "y = np.concatenate([y, y], axis=0)\n",
    "X_features.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset \n",
    "\n",
    "It is important to split the data subject-wise as otherwise data from testing voxels is leaking into training voxels via the rolling window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_images, n_x, n_y, n_z, n_features = X_features.shape\n",
    "X_flat = X_features.reshape(n_images, -1, n_features)\n",
    "y_flat = y.reshape(n_images, -1)\n",
    "# test_size is 0.5 so that the pipeline can be used with only 2 subjects\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y_flat, test_size=0.5, random_state=42)\n",
    "X_train, y_train = X_train.reshape(-1, n_features), y_train.reshape(-1)\n",
    "X_test, y_test = X_test.reshape(-1, n_features), y_test.reshape(-1)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "probas = classifier.predict_proba(X_test)\n",
    "predicted = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "probas_3D = probas.reshape(-1, n_x, n_y, n_z, 2)\n",
    "predicted_3D = predicted.reshape(-1, n_x, n_y, n_z)\n",
    "probas_3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(probas_3D[...,1])\n",
    "display(predicted_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Features + Classifier) Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from analysis_tools.metrics.plot_ROC import plot_roc\n",
    "\n",
    "def roc_auc(label_gt, label_pred):\n",
    "    y_true = np.array(label_gt).flatten()\n",
    "    y_scores = np.array(label_pred).flatten()\n",
    "\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc_score = auc(fpr, tpr)\n",
    "    return roc_auc_score, (fpr, tpr, roc_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(im1, im2, empty_score=1.0):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient, a measure of set similarity.\n",
    "    Parameters\n",
    "    ----------\n",
    "    im1 : array-like, bool\n",
    "        Any array of arbitrary size. If not boolean, will be converted.\n",
    "    im2 : array-like, bool\n",
    "        Any other array of identical size. If not boolean, will be converted.\n",
    "    Returns\n",
    "    -------\n",
    "    dice : float\n",
    "        Dice coefficient as a float on range [0,1].\n",
    "        Maximum similarity = 1\n",
    "        No similarity = 0\n",
    "        Both are empty (sum eq to zero) = empty_score\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The order of inputs for `dice` is irrelevant. The result will be\n",
    "    identical if `im1` and `im2` are switched.\n",
    "    \"\"\"\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
    "\n",
    "    im_sum = im1.sum() + im2.sum()\n",
    "    if im_sum == 0:\n",
    "        return empty_score\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return 2. * intersection.sum() / im_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score = dice(predicted.flatten(), y_test.flatten())\n",
    "roc_auc_score, roc_curve_details = roc_auc(y_test, predicted)\n",
    "\n",
    "print('Dice:', dice_score)\n",
    "print('ROC AUC:', roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_thresholds = roc_curve_details\n",
    "plot_roc([tpr], [fpr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, predicted)\n",
    "plt.imshow(confusion)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature correalation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "correlation = np.abs(np.corrcoef(X_train.T))\n",
    "plt.imshow(correlation)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (BnDs)",
   "language": "python",
   "name": "pycharm-f52dae77"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}